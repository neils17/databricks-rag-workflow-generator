"""
Working RAG Client for Databricks Integration
Generated by Fixed Enhanced Production Workflow
This version works with available Databricks assets
"""

import os
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

@dataclass
class WorkingRAGConfig:
    """Configuration for working RAG system"""
    workflow_id: str = "20250813_132627"
    vector_search_endpoint: str = "rag_endpoint_20250813_132627"
    embedding_model: str = "text-embedding-3-small"
    chunk_size: int = 1000
    chunk_overlap: int = 200
    top_k: int = 5

class WorkingDatabricksRAGClient:
    """Working client for interacting with Databricks RAG system"""
    
    def __init__(self, config: WorkingRAGConfig):
        self.config = config
        self.host = os.getenv("DATABRICKS_HOST")
        self.token = os.getenv("DATABRICKS_TOKEN")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
        # Initialize LangChain components
        self.embeddings = OpenAIEmbeddings(
            model=config.embedding_model,
            openai_api_key=self.openai_api_key
        )
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.1,
            openai_api_key=self.openai_api_key
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=len,
        )
        
        print(f"Working RAG Client initialized for workflow: {config.workflow_id}")
    
    def get_workspace_url(self) -> str:
        """Get the workspace URL for viewing assets"""
        return f"{self.host}/workspace/Shared/Working_RAG_Workflow_{self.config.workflow_id}"
    
    def process_documents(self, documents: List[str]) -> List[Document]:
        """Process raw documents into LangChain Document objects"""
        docs = []
        for i, doc_text in enumerate(documents):
            chunks = self.text_splitter.split_text(doc_text)
            for j, chunk in enumerate(chunks):
                docs.append(Document(
                    page_content=chunk,
                    metadata={"source": f"document_{i}", "chunk": j}
                ))
        return docs
    
    def query(self, query: str, documents: List[str]) -> str:
        """Perform RAG query with full implementation"""
        try:
            # Process documents
            processed_docs = self.process_documents(documents)
            
            # Create embeddings for query
            query_embedding = self.embeddings.embed_query(query)
            
            # Simple similarity search
            similarities = []
            for doc in processed_docs:
                doc_embedding = self.embeddings.embed_query(doc.page_content)
                # Calculate cosine similarity (simplified)
                similarity = sum(a * b for a, b in zip(query_embedding, doc_embedding))
                similarities.append((similarity, doc))
            
            # Sort by similarity and get top k
            similarities.sort(key=lambda x: x[0], reverse=True)
            top_docs = [doc for _, doc in similarities[:self.config.top_k]]
            
            # Prepare context from retrieved documents
            context = "\n\n".join([doc.page_content for doc in top_docs])
            
            # Create prompt for LLM
            prompt = f"""Based on the following context, answer the question. If the context doesn't contain enough information to answer the question, say so.

Context:
{context}

Question: {query}

Answer:"""
            
            # Generate response using LLM
            response = self.llm.invoke(prompt)
            
            return response.content
            
        except Exception as e:
            return f"Error performing RAG query: {str(e)}"
    
    def get_asset_status(self) -> Dict[str, Any]:
        """Get status of Databricks assets"""
        try:
            assets_status = {
                "workflow_id": self.config.workflow_id,
                "workspace_url": self.get_workspace_url(),
                "assets": {
                    "vector_search_endpoint": self.config.vector_search_endpoint,
                    "folder": "Shared/Working_RAG_Workflow_{self.config.workflow_id}"
                }
            }
            return assets_status
        except Exception as e:
            return {"error": str(e)}

# Example usage
if __name__ == "__main__":
    config = WorkingRAGConfig()
    client = WorkingDatabricksRAGClient(config)
    
    # Check asset status
    status = client.get_asset_status()
    print("Asset Status:", status)
    
    # Example query with full implementation
    sample_docs = ["Machine learning is a subset of artificial intelligence..."]
    result = client.query("What is machine learning?", sample_docs)
    print(result)
